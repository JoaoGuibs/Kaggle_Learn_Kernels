{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49037,
     "status": "ok",
     "timestamp": 1569060998453,
     "user": {
      "displayName": "Joao Guilherme",
      "photoUrl": "",
      "userId": "02077358952033531630"
     },
     "user_tz": -60
    },
    "id": "NXNpIAc0UkRB",
    "outputId": "c6d42503-3f93-4de6-efdb-14840c6082b3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-578115223675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88930,
     "status": "ok",
     "timestamp": 1569061192377,
     "user": {
      "displayName": "Joao Guilherme",
      "photoUrl": "",
      "userId": "02077358952033531630"
     },
     "user_tz": -60
    },
    "id": "R42JUCPZU5A-",
    "outputId": "4487cc6c-1ce2-4e95-f3d4-283727e5cbd7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5ed94657ce83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Imports:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGaussianNoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "#Imports:\n",
    "import numpy as numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten,Dropout, GaussianNoise\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw \n",
    "##Functions::\n",
    "\n",
    "def create_cnn_model():\n",
    "    \"\"\"\n",
    "    This functions creates the Deep Learning Model used. Our model has 4 convolution layers, each\n",
    "    always followed by a maxpooling layer. We apply dropout twice to prevent overfitting.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D( 32, (2,2) , padding='same' , input_shape = (80,80,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2) )) #40x40\n",
    "    model.add(Conv2D( 32, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(MaxPooling2D( pool_size= (2,2) )) #20x20\n",
    "    model.add(Conv2D( 64, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2) )) #10x10\n",
    "    model.add(Conv2D( 64, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2) )) #5x5\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024 , activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2 , activation = 'softmax'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data(input_file):\n",
    "    \n",
    "    with open(input_file, 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "\n",
    "    # parse file\n",
    "    obj = json.loads(data)\n",
    "    dict_labels = obj.keys()\n",
    "\n",
    "    print('The dictionary labels are: ', dict_labels)\n",
    "\n",
    "    number_ships = np.count_nonzero(obj['labels'])\n",
    "    print('The number of ships in the data is: ', number_ships)\n",
    "    print('The total number of images is: ', len(obj['data']))\n",
    "\n",
    "    return obj\n",
    "\n",
    "def not_near(x, y, s, coordinates):\n",
    "    \"\"\"\n",
    "        Function that prevents that we have similar images, due to the small step value.\n",
    "    \"\"\"\n",
    "    result = True\n",
    "    for e in coordinates:\n",
    "        if x+s > e[0][0] and x-s < e[0][0] and y+s > e[0][1] and y-s < e[0][1]:\n",
    "            result = False\n",
    "    return result\n",
    "\n",
    "def draw_box(scene, coord):\n",
    "    \"\"\"\n",
    "        Draws the object box in the derived location of the object.\n",
    "    \"\"\"\n",
    "    scene[coord[0], coord[1]:coord[1] + 80 , 0:3 ] = 0\n",
    "    scene[coord[0]:coord[0] + 80, coord[1] , 0:3 ] = 0\n",
    "    scene[coord[0] + 80, coord[1]:coord[1] + 80 , 0:3 ] = 0\n",
    "    scene[coord[0]:coord[0] + 80, coord[1] + 80 , 0:3 ] = 0\n",
    "\n",
    "    return scene\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_scene(input_file, model):\n",
    "    \"\"\"\n",
    "    We evaluate a full satellite image, looking for the locations\n",
    "    of the ships. \n",
    "    \"\"\"\n",
    "    #Open the scene that will be evaluated\n",
    "    scene = Image.open(input_file)\n",
    "    scene = np.array(scene).astype(np.uint8)\n",
    "    scene = scene / 255 #We normalize the input data\n",
    "    \n",
    "    #Plot the original scene\n",
    "    plt.figure()\n",
    "    plt.title('Input Satellite Image')\n",
    "    plt.imshow(scene)\n",
    "    plt.show()\n",
    "    \n",
    "    image_size = (80,80,3) #Harcoded image size\n",
    "    N_tiles = 0\n",
    "    #The step is a tolerance that allows us to better obtain good images of the boats\n",
    "    step = 10\n",
    "    coordinates = []\n",
    "    scene_with_boxes = scene.copy()\n",
    "    \n",
    "    for i in range(0, int((scene.shape[0]-(80-step))/step )):\n",
    "        \n",
    "        for j in range(0, int((scene.shape[1] - (80-step))/step )):\n",
    "            print(i, j)\n",
    "            #Create a 80x80 tile, and add the info on the scene\n",
    "            small_tile = np.zeros((1,80,80,3))\n",
    "            small_tile[0] = scene[ i*step:i*step + 80,  j*step : j*step + 80 , 0:3]\n",
    "\n",
    "            #Predict if there is a visible boat            \n",
    "            prediction = model.predict(small_tile)\n",
    "\n",
    "            #Prediction Tolerance HARDCODED\n",
    "            if(prediction[0][1] > 0.9 and not_near(i*step,j*step, 88, coordinates) ):\n",
    "                \n",
    "                coordinates.append([[i*step, j*step], prediction])\n",
    "                draw_box(scene_with_boxes, coordinates[-1][0])\n",
    "                \n",
    "    #Plot the scene, with the object's boxes drawn\n",
    "    plt.figure()\n",
    "    plt.imshow(scene_with_boxes)\n",
    "    plt.show()\n",
    "    scene_with_boxes *= 255;\n",
    "    \n",
    "    im = Image.fromarray(scene_with_boxes.astype(np.uint8))\n",
    "    im.save(os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/Outputs/'+'sfbay_1.png', 'PNG')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_results(history):\n",
    "    #Obtains the training history and plots the accuracy and loss\n",
    "    acc = history.history['acc']\n",
    "    loss = history.history['loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(acc, label = 'Train Accuracy')\n",
    "    plt.plot(val_acc, label = 'Validation Accuracy')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(loss, label = 'Train Loss')\n",
    "    plt.plot(val_loss, label = 'Validation Loss')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "###Main Program:\n",
    "def main():\n",
    "\n",
    "    is_model_trained = input('Is there a trained model which you want to use? [y/n]')\n",
    "\n",
    "    if(is_model_trained != 'y'):\n",
    "        print('Training a New Model')\n",
    "        print(os.getcwd())\n",
    "        input_file = os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/ships_data/shipsnet.json'\n",
    "        json_obj = read_json_data(input_file)\n",
    "\n",
    "        cnn_model = create_cnn_model()\n",
    "        #sgd = SGD(lr = 0.01, momentum = 0.9, nesterov=True)\n",
    "        cnn_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        N_images = len(json_obj['data'])\n",
    "        input_data = np.array(json_obj['data']).astype(np.uint8).reshape((N_images, 3, 80, 80))\n",
    "        input_data = input_data/255\n",
    "        label_data = to_categorical(np.array(json_obj['labels']).astype(np.uint8),2)\n",
    "        \n",
    "        print('Input data and data labels read correctly')\n",
    "        indexes = np.arange(N_images)\n",
    "        #randomly shuffle and reshape input data (N_images, nrows, ncols, channel)\n",
    "        np.random.shuffle(indexes)\n",
    "        validation_split = 0.2\n",
    "        #We create validation data. The amount is a percentage of the train data that is removed \n",
    "        #and is not used for training\n",
    "        shuffled_input_data = input_data[indexes].transpose( [0,2,3,1] )\n",
    "        shuffled_val_data = shuffled_input_data[0: int(validation_split * shuffled_input_data.shape[0]) ]\n",
    "        shuffled_input_data = shuffled_input_data[int(validation_split * shuffled_input_data.shape[0]):]\n",
    "        \n",
    "        #TODO: Understand the to_categorical command\n",
    "        shuffled_label_data = label_data[indexes]\n",
    "        shuffled_val_label_data = shuffled_label_data[0: int(validation_split * shuffled_label_data.shape[0]) ]\n",
    "        shuffled_label_data = shuffled_label_data[int(validation_split * shuffled_label_data.shape[0]):]\n",
    "\n",
    "        #Train the created model, with the input and label data, 20% of data is used for validation\n",
    "        batch_size = 32\n",
    "        epochs = 25\n",
    "        #We create a train generator, where we apply data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "                    featurewise_center = True,\n",
    "                    featurewise_std_normalization = True,\n",
    "                    rotation_range = 20,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    horizontal_flip = True)\n",
    "\n",
    "        valgen = ImageDataGenerator()\n",
    "        input_images = datagen.flow(shuffled_input_data, shuffled_label_data, batch_size = 32)\n",
    "        validation_images = valgen.flow(shuffled_val_data, shuffled_val_label_data )\n",
    "        history = cnn_model.fit_generator(input_images, epochs = epochs , verbose = 2, validation_data = validation_images )\n",
    "        #Save the trained model\n",
    "        cnn_model.save(os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/models/ships_cnn_model_b\"%s\"_e\"%s\".h5'%(str(batch_size), str(epochs)) )\n",
    "        #Show the training and validation results over each epoch\n",
    "        plot_train_results(history)\n",
    "    else:\n",
    "        print('Loading Trained model')\n",
    "        cnn_model = keras.models.load_model(os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/models/ships_cnn_model_b\"32\"_e\"25\".h5')\n",
    "    \n",
    "    scene_file = input('What is the scene file name? ')\n",
    "    scene_data = os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/ships_data/scenes/'+scene_file\n",
    "    \n",
    "    evalute_scene(scene_data, cnn_model)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print(local_device_protos)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ships_Satellite.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
