{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22454,
     "status": "ok",
     "timestamp": 1570005047060,
     "user": {
      "displayName": "Joao Guilherme",
      "photoUrl": "",
      "userId": "02077358952033531630"
     },
     "user_tz": -60
    },
    "id": "NXNpIAc0UkRB",
    "outputId": "83bf65d7-aa35-43d8-b563-721dfcb3766f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R42JUCPZU5A-"
   },
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import numpy as numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, GaussianNoise, BatchNormalization, Cropping2D\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw \n",
    "import re\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Functions::\n",
    "def create_cnn_model():\n",
    "    \"\"\"\n",
    "    This functions creates the Deep Learning Model used. Our model has 4 \n",
    "    convolution layers, each almost always followed by a maxpooling layer. \n",
    "    We apply dropout once and batch normalization to prevent overfitting.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    model = keras.models.Sequential()\n",
    "    #model.add(GaussianNoise(0.015, input_shape = (80,80,3)))\n",
    "    model.add(Conv2D( 32, (3,3) , padding='same' , input_shape = (80,80,3),  activation = 'relu'))\n",
    "    model.add(Conv2D( 32, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D( pool_size = (2,2) )) #40x40\n",
    "    model.add(Conv2D( 64, (3,3) , padding='same' , input_shape = (80,80,3),  activation = 'relu'))\n",
    "    model.add(Conv2D( 64, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D( pool_size= (2,2) )) #20x20\n",
    "    model.add(Conv2D( 128, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(Conv2D( 128, (3,3) , padding='same' , activation = 'relu' ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = (2,2) )) #10x10\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024 , activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2 , activation = 'softmax'))\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUBKNcGsmIQd"
   },
   "outputs": [],
   "source": [
    "def read_json_data(input_file):\n",
    "    \"\"\"\n",
    "      This function reads the input file's json data.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "\n",
    "    # parse file\n",
    "    obj = json.loads(data)\n",
    "    dict_labels = obj.keys()\n",
    "\n",
    "    print('The dictionary labels are: ', dict_labels)\n",
    "\n",
    "    number_ships = np.count_nonzero(obj['labels'])\n",
    "    print('The number of ships in the data is: ', number_ships)\n",
    "    print('The total number of images is: ', len(obj['data']))\n",
    "\n",
    "    return obj\n",
    "\n",
    "###Function taken from: https://www.kaggle.com/byrachonok/keras-for-search-ships-in-satellite-image\n",
    "def not_near(x, y, s, coordinates):\n",
    "    \"\"\"\n",
    "        Function that prevents that we have similar images, due to the small step value.\n",
    "    \"\"\"\n",
    "    result = True\n",
    "    for e in coordinates:\n",
    "        if x+s > e[0][0] and x-s < e[0][0] and y+s > e[0][1] and y-s < e[0][1]:\n",
    "            result = False\n",
    "    return result\n",
    "\n",
    "def draw_box(scene, coord):\n",
    "    \"\"\"\n",
    "        Draws the object box in the derived location of the object.\n",
    "        We put the box thicker so it will be more easily seen.\n",
    "    \"\"\"\n",
    "    scene[coord[0]: coord[0] + 3, coord[1] : coord[1] + 80 , 0:3 ] = 0\n",
    "    \n",
    "    scene[coord[0]: coord[0] + 80, coord[1]: coord[1] + 3 , 0:3 ] = 0\n",
    "    \n",
    "    scene[coord[0] + 80 : coord[0] + 83, coord[1] : coord[1] + 80 , 0:3 ] = 0\n",
    "    \n",
    "    scene[coord[0]:coord[0] + 80, coord[1] + 80 : coord[1] + 83 , 0:3 ] = 0\n",
    "\n",
    "    return scene\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOqEQJmFmIQk"
   },
   "outputs": [],
   "source": [
    "def evalute_scene(input_file, model, scene_file):\n",
    "    \"\"\"\n",
    "    We evaluate a full satellite image, looking for the \n",
    "    approximate locations of the ships in 80x80 ROI.\n",
    "    \"\"\"\n",
    "    #Open the scene that will be evaluated\n",
    "    scene = Image.open(input_file)\n",
    "    scene = np.array(scene).astype(np.uint8)\n",
    "    scene = scene / 255 #We normalize the input data\n",
    "    \n",
    "    #Plot the original scene\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.title('Input Satellite Image')\n",
    "    plt.imshow(scene)\n",
    "    plt.show()\n",
    "    \n",
    "    image_size = (80,80,3) #Harcoded image size\n",
    "    \n",
    "    #The step is a tolerance that allows us to better obtain good images of the boats\n",
    "    step = 10\n",
    "    coordinates = []\n",
    "    scene_with_boxes = scene.copy()\n",
    "    \n",
    "    for i in range(0, int((scene.shape[0]-(image_size[0] - step))/step )):\n",
    "        \n",
    "        for j in range(0, int((scene.shape[1] - (image_size[1] - step))/step )):\n",
    "            #Create a 80x80 tile, and add the info on the scene\n",
    "            small_tile = np.zeros((1, image_size[0], image_size[1], image_size[2]))\n",
    "            small_tile[0] = scene[ i*step:i*step + image_size[0],  j*step : j*step + image_size[1] , 0:image_size[2]]\n",
    "\n",
    "            #Predict if there is a visible boat            \n",
    "            prediction = model.predict(small_tile)\n",
    "\n",
    "            #Prediction Tolerance HARDCODED\n",
    "            if(prediction[0][1] > 0.9 and not_near(i*step,j*step, 88, coordinates) ):\n",
    "                print('A ship was found, with confidence = %.5f'%(prediction[0][1]))\n",
    "                coordinates.append([[i*step, j*step], prediction])\n",
    "                draw_box(scene_with_boxes, coordinates[-1][0])\n",
    "                \n",
    "    #Plot the scene, with the object's boxes drawn\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.title('Output scene, with bounding boxes')\n",
    "    plt.imshow(scene_with_boxes)\n",
    "    plt.show()\n",
    "    scene_with_boxes *= 255\n",
    "    \n",
    "    im = Image.fromarray(scene_with_boxes.astype(np.uint8))\n",
    "    im.save(os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/Outputs/'+scene_file, 'PNG')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_results(history):\n",
    "    \"\"\"\n",
    "    Obtains the training history and plots the accuracy and loss\n",
    "    of both the training and validation.\n",
    "    \"\"\"\n",
    "    acc = history.history['acc']\n",
    "    loss = history.history['loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize = (20,20) )\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(acc, label = 'Train Accuracy')\n",
    "    plt.plot(val_acc, label = 'Validation Accuracy')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(loss, label = 'Train Loss')\n",
    "    plt.plot(val_loss, label = 'Validation Loss')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1GFN2SJq9IiQsEisTkz_iu-vR30gYJFg_"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 482754,
     "status": "ok",
     "timestamp": 1570006730054,
     "user": {
      "displayName": "Joao Guilherme",
      "photoUrl": "",
      "userId": "02077358952033531630"
     },
     "user_tz": -60
    },
    "id": "QDiFKtz3mIQq",
    "outputId": "36ff2d84-5c43-4e2a-d00d-a3ad333b10f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Main Program:\n",
    "def main():\n",
    "\n",
    "    is_model_trained = input('Is there a trained model which you want to use? [y/n]')\n",
    "    \n",
    "    base_dir = os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/'\n",
    "    \n",
    "    if(is_model_trained != 'y'):\n",
    "        print('Training a New Model')\n",
    "        \n",
    "        input_file = os.getcwd()+'/gdrive/My Drive/Colab Notebooks/Ships_ML/ships_data/shipsnet.json'\n",
    "        json_obj = read_json_data(input_file)\n",
    "        cnn_model = create_cnn_model()\n",
    "        \n",
    "        ###OPTIONAL\n",
    "        \"\"\"\n",
    "        use_vgg19 = input('Do you want to use the first layers of the VGG 19 model? [y/n]')\n",
    "        if(use_vgg19 == 'y'):\n",
    "          vgg19_model = VGG19(include_top = False, weights='imagenet')\n",
    "          \n",
    "          ##DEBUG\n",
    "          for lay in vgg19_model.layers:\n",
    "            print(lay.name)\n",
    "          ##END DEBUG\n",
    "          \n",
    "          cnn_model.layers[0].set_weights = vgg19_model.layers[1].get_weights()\n",
    "          cnn_model.layers[2].set_weights = vgg19_model.layers[2].get_weights()\n",
    "        \"\"\"\n",
    "        ###END OPTIONAL\n",
    "        \n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        cnn_model.compile(optimizer = Adam(1e-3, decay=1e-6), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        N_images = len(json_obj['data'])\n",
    "        input_data = np.array(json_obj['data']).astype(np.uint8).reshape((N_images, 3, 80, 80))\n",
    "        input_data = input_data/255\n",
    "        label_data = to_categorical(np.array(json_obj['labels']).astype(np.uint8),2)\n",
    "        \n",
    "        print('Input data and data labels read correctly')\n",
    "        indexes = np.arange(N_images)\n",
    "        #randomly shuffle and reshape input data (N_images, nrows, ncols, channel)\n",
    "        np.random.shuffle(indexes)\n",
    "        validation_split = 0.2\n",
    "        #We create the validation data. Its amount is a percentage of the train data\n",
    "        #that is removed and is not used for training\n",
    "        shuffled_input_data = input_data[indexes].transpose( [0,2,3,1] )\n",
    "        \n",
    "        shuffled_val_data = shuffled_input_data[0:int(validation_split * shuffled_input_data.shape[0]) ]\n",
    "        shuffled_input_data = shuffled_input_data[int(validation_split * shuffled_input_data.shape[0]):]\n",
    "        \n",
    "        shuffled_label_data = label_data[indexes]\n",
    "        shuffled_val_label_data = shuffled_label_data[0: int(validation_split * shuffled_label_data.shape[0]) ]\n",
    "        shuffled_label_data = shuffled_label_data[int(validation_split * shuffled_label_data.shape[0]):]\n",
    "\n",
    "        #Train the created model, with the input and label data, 20% of data is used for validation\n",
    "        batch_size = 32\n",
    "        epochs = 100\n",
    "        #We create a train generator, where we apply data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "                    rotation_range = 40,\n",
    "                    width_shift_range = 0.3,\n",
    "                    height_shift_range = 0.3,\n",
    "                    zoom_range = 0.5,\n",
    "                    horizontal_flip = True)\n",
    "\n",
    "        valgen = ImageDataGenerator()\n",
    "        input_images = datagen.flow(shuffled_input_data, shuffled_label_data, batch_size = batch_size)\n",
    "        validation_images = valgen.flow(shuffled_val_data, shuffled_val_label_data )\n",
    "        \n",
    "        out_file = base_dir + 'models/ships_cnn_model_b\"%s\"_e\"%s\"'%(str(batch_size), str(epochs)) +'_{epoch:02d}-{val_loss:.2f}.h5'\n",
    "        #Define the callbacks used in training\n",
    "        reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2,\n",
    "                              patience = 3, min_lr = 0.0001)\n",
    "        save_best = ModelCheckpoint(out_file, monitor='val_loss', verbose = 0, save_best_only=True)\n",
    "        \n",
    "        history = cnn_model.fit_generator(input_images, epochs = epochs , verbose = 2, validation_data = validation_images, callbacks = [reduce_lr, save_best] )\n",
    "        \n",
    "        #Show the training and validation results over each epoch\n",
    "        plot_train_results(history)\n",
    "        return 0\n",
    "    else:\n",
    "        print('Loading Trained model')\n",
    "        cnn_model = keras.models.load_model(base_dir + 'models/ships_cnn_model_b\"32\"_e\"100\"_51-0.11.h5')\n",
    "    \n",
    "        scene_list = glob.glob(base_dir + '/ships_data/scenes/*.png')[0:2]\n",
    "\n",
    "\n",
    "        #scene_file = input('What is the scene file name? ')\n",
    "        for scene_data in scene_list:\n",
    "          scene_file = re.sub(base_dir + '/ships_data/scenes/', '', scene_data )\n",
    "\n",
    "          evalute_scene(scene_data, cnn_model, scene_file)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print(local_device_protos)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Ships_Satellite.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/JoaoGuibs/Kaggle_Learn_Kernels/blob/master/ships-and-planes-in-satellite-imagery/Ships_Satellite.ipynb",
     "timestamp": 1569086951149
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
